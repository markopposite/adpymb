{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script to conjoin data tables from the separated raw data table output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'inspect' has no attribute 'isclass'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggr_df(lake_id, category, dir1, dir2, sort_key = \"time\"): \n",
    "    \"\"\" Takes in csv's for a lake and category and returns a stacked table\n",
    "     \n",
    "    Parameters\n",
    "    ---------\n",
    "    lake_id : str\n",
    "        OWS19, SEN18, SKN19... ; identify the lake and the year matching raw data files\n",
    "    category : str\n",
    "        amp_av, vel_N_W, corr_bm_1...; many categories exist for each raw data file\n",
    "    dir1 : str\n",
    "        directory where the separated data tables are located\n",
    "    dir2 : str\n",
    "        directory where the stacked data tables will end up\n",
    "    format examples: \n",
    "        dir1 = \"adcp_habs/data/adcp_data_tables/\"\n",
    "        dir2 = \"/home/mpoe/adcp_habs/data/adcp_tables_stacked/\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    csv\n",
    "        one large table written to a new file and read back in\n",
    "    \"\"\"\n",
    "    \n",
    "    # directory business\n",
    "    file_directory = dir1 + lake_id + \"*/*\" + category + \"*.csv\"\n",
    "    pt_dir = dir2 + lake_id\n",
    "    pt_file = pt_dir + \"/\" + lake_id + \"_\" + category + \".csv\"\n",
    "    # define in pathlib format and write the directory if not exist\n",
    "    point_directory = Path.home()/Path(pt_dir)\n",
    "    point_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # pull the files and create list of df's\n",
    "    df_list = []\n",
    "    for file_path in Path.home().glob(file_directory):\n",
    "        indiv_csv = pd.read_csv(file_path)\n",
    "        df_list.append(indiv_csv)\n",
    "\n",
    "    # change all dfs in list to have matching column names\n",
    "    cols = df_list[0].columns\n",
    "    for i in df_list:\n",
    "        i.columns = cols\n",
    "\n",
    "    # stack df's into one table and sort by time\n",
    "    concat_df = pd.concat(df_list, ignore_index=True)\n",
    "    concat_df = concat_df.sort_values(sort_key, ascending=True) \n",
    "\n",
    "    # write df as csv to disk and read back in\n",
    "    concat_df.to_csv(pt_file, index=False)\n",
    "    csv_read = pd.read_csv(pt_file)\n",
    "\n",
    "    return (csv_read)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation; stacked tables are built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general directory structure; \n",
    "name1 = \"adcp_habs/data/adcp_data_tables/\"\n",
    "name2 = \"/home/mpoe/adcp_habs/data/adcp_tables_stacked/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array for looping through all of the categories and build all tables in one go;\n",
    "# recommended: only run one lake*year at a time for categories list\n",
    "categories = [\"amp_avg\", \"amp_beam1\", \"amp_beam2\", \"amp_beam3\", \"amp_beam4\", \"corr_bm1\", \"corr_bm2\", \"corr_bm3\", \"corr_bm4\",\n",
    "              \"prcnt_good_bm1\", \"prcnt_good_bm2\", \"prcnt_good_bm3\", \"prcnt_good_bm4\", \"table_time_series\", \"vel_E_W\", \n",
    "              \"vel_err\", \"vel_N_S\", \"vel_x_vrt\"]\n",
    "# for building bins table, loop through all lake*years okay for one category\n",
    "layk_idees = [\"OWS19\", \"OWS20\", \"SEN18\", \"SEN19\", \"SKN19\", \"SEN20\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a single lake*year: tables for all categories\n",
    "for i in categories: \n",
    "    tester = aggr_df(\"SEN19\", i, name1, name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single category for all lake*years\n",
    "for i in layk_idees:\n",
    "    aggr_df(i, \"table_bins\", name1, name2, sort_key = \"bin_depth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggr_df(\"OWS19\", \"table_bins\", name1, name2, sort_key=\"bin_depth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ju_py_r",
   "language": "python",
   "name": "ju_py_r"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
